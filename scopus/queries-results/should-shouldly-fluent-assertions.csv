Authors,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Link,Abstract,Author Keywords,Index Keywords,Publisher
"Derezińska A., Rudnik M.","Quality evaluation of object-oriented and standard mutation operators applied to C# programs",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7304 LNCS",,,"42","57",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862175310&doi=10.1007%2f978-3-642-30561-0_5&partnerID=40&md5=518a13f8fbca7175a9e75ff0d247d0a9","Mutation testing is a kind of fault injection approach that can be used to generate tests or to assess the quality of test sets. For object-oriented languages, like C#, both object-oriented and standard (traditional) mutation operators should be applied. The methods that can contribute to reducing the number of applied operators and lowering the costs of mutation testing were experimentally investigated. We extended the CREAM mutation tool to support selective testing, sampling and clustering of mutants, and combining code coverage with mutation testing. We propose an approach to quality evaluation and present experimental results of mutation operators applied to C# programs. © 2012 Springer-Verlag.","C#; mutation testing; object-oriented mutation operators","C#; Code coverage; Fault injection; Mutation operators; Mutation testing; Object oriented; Object-oriented languages; Quality evaluation; Selective testing; Test sets; Software testing; Quality control",
"Oyetoyan T.D., Conradi R., Cruzes D.S.","Criticality of defects in cyclic dependent components",2013,"IEEE 13th International Working Conference on Source Code Analysis and Manipulation, SCAM 2013",,,"6648180","21","30",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891120922&doi=10.1109%2fSCAM.2013.6648180&partnerID=40&md5=e1617cab884c769dcda6d32c66cf1be4","Background: Software defects that most likely will turn into system and/or business failures are termed critical by most stakeholders. Thus, having some warnings of the most probable location of such critical defects in a software system is crucial. Software complexity (e.g. coupling) has long been established to be associated with the number of defects. However, what is really challenging is not in the number but identifying the most severe defects that impact reliability. (Research Goal) Do cyclic related components account for a clear majority of the critical defects in software systems? Approach: We have empirically evaluated two non-trivial systems. One commercial Smart Grid system developed with C# and an open source messaging and integrated pattern server developed with Java. By using cycle metrics, we mined the components into cyclic-related and non-cyclic related groups. Lastly, we evaluated the statistical significance of critical defects and severe defect-prone components (SDCs) in both groups. Results: In these two systems, results demonstrated convincingly, that components in cyclic relationships account for a significant and the most critical defects and SDCs. Discussion and Conclusion: We further identified a segment of a system with cyclic complexity that consist almost all of the critical defects and SDCs that impact on system's reliability. Such critical defects and the affected components should be focused for increased testing and refactoring possibilities. © 2013 IEEE.","defect distribution; defect severity; defect-prone components; dependency cycles; empirical study; software reliability","Computer software; Defects; Plant shutdowns; Software reliability; Defect distribution; defect-prone components; dependency cycles; Empirical studies; Integrated pattern; Smart grid systems; Software complexity; Statistical significance; Open systems","IEEE Computer Society"
"Li H., Wu C.","Study on the application of digital certificates in the protection of network information security and data integrity",2013,"Journal of Networks","8","11",,"2592","2598",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885904148&doi=10.4304%2fjnw.8.11.2592-2598&partnerID=40&md5=1c7dc23ad495dc307e0abd5aecb40125","With the development of Internet technology, Internet has entered each field of society. At the same time, the popularity of network has led to the some illegal activities which are implemented by using network vulnerabilities. In addition, the risk of valuable information's being abused has drastically increased. Thus, it is one of the pressing tasks at issue that we should solve the security of Internet information and provide a reliable Internet information security policy. First of all, this article analyzes the application system framework of digital certificate and its safety features, and then combined with the certificate of data frame and the body's immune model, this paper builds the multidimensional network information security platform framework on the basis of digital certificates. By doing this, the client safety and the master-slave structure security have been effectively solved. Next, this paper makes a case analysis. Furthermore, digital certificate is innovatively applied to data nodes of the network information data integrity protection. According to the markov model, this paper constructs the model of data integrity based on digital certificate and then this paper makes test and simulation on the accuracy of the model through the VB. NET programming. The result shows that the model has obvious effect on data integrity protection. To some extent, it provides theory and practice support for the research in this field. © 2013 ACADEMY PUBLISHER.","Data integrity; Digital certificate; Immune model; Information security; Markov; Simulation","Data integrity; Digital certificates; Immune models; Markov; Simulation; Computer simulation; Internet; Markov processes; Safety engineering; Security of data",
"Okuyan E., Güdükbay U.","BilKristal 2.0: A tool for pattern information extraction from crystal structures",2014,"Computer Physics Communications","185","1",,"442","443",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932193006&doi=10.1016%2fj.cpc.2013.09.020&partnerID=40&md5=3f98b11dba1603f40068058efe060e8b","We present a revised version of the BilKristal tool of Okuyan et al. (2007). We converted the development environment into Microsoft Visual Studio 2005 in order to resolve compatibility issues. We added multi-core CPU support and improvements are made to graphics functions in order to improve performance. Discovered bugs are fixed and exporting functionality to a material visualization tool is added. New version program summary Program title: BilKristal 2.0. Catalogue identifier: ADYU_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/ADYU_v2_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 364263 No. of bytes in distributed program, including test data, etc.: 9135815 Distribution format: tar.gz Programming language: C, C++, Microsoft.NET Framework 2.0 and OpenGL Libraries. Computer: Personal computer with Windows operating system. Operating system: Windows XP or higher. Has the code been vectorized or parallelized?: Multi-core CPU support included. RAM: 20–60 Megabytes. Catalogue identifier of previous version: ADYU_v1_0 Journal reference of previous version: Comput. Phys. Comm. 176 (2007) 486 Classification: 8. External routines: Microsoft.NET Framework 2.0. For the visualization tool, the graphics card driver should also support OpenGL. Does the new version supercede the previous version?: Yes Nature of problem: Determining the crystal structure parameters of a material is a very important issue in crystallography. Knowing the crystal structure parameters helps the understanding of the physical behavior of a material. For complex structures, particularly for materials which also contain local symmetry as well as global symmetry, obtaining crystal parameters can be very hard. Solution method: The tool extracts crystal parameters such as primitive vectors and basis vectors and identifies the space group from the atomic coordinates of crystal structures. Reasons for new version: Additional features, resolved compatibility issues with the new development environments, performance optimizations, minor bug corrections. Summary of revisions: • Capability to export to MaterialVis tool [1] is added. The tool can export the unit cell information extracted from the crystal structure, the raw atomic coordinates and atomic radii into a data file (.dat) that the MaterialVis tool can process.• Compatibility issues with Microsoft Visual Studio 2005 up to 2010 are resolved. The original code was developed using Microsoft Visual Studio 2003. However, newer Visual Studio versions were not able to convert and compile the code. Due to the changes in the.NET framework, the converted project produced many errors. In this work, the project is converted into a Visual Studio 2005 project and compilation errors are resolved. We also tested the code with Visual Studio 2010 and the project was successfully converted and compiled.• Multi-Core CPU support is added. In recent years, multi-core CPUs have become very common. We added the multi-core CPU support in order to utilize the computational capabilities of additional CPU cores. This significantly improves the performance.• The visualization interface is improved. In particular, the sphere drawing functionality is replaced with an efficient and high quality version that utilizes GPU acceleration.• For some cases, the fractional coordinates of some of the calculated basis vectors were not all in the [0,1) range, but at coordinate 1.0 for some axes. These cases were corrected by translating these basis vectors into the [0,1) range. Restrictions: Assumptions are explained in [2]. However, none of them can be considered as a restriction on the complexity of the problem. Running time: The tool was able to process input files with more than a million atoms in less than 20 s on a PC with an Athlon quad-core CPU at 3.2 GHz using the default parameter values. References: [1] Erhan Okuyan, Ugur Güdükbay, MaterialVis: Crystal and Amorphous Material Visualization Tool Using Direct Volume and Surface Rendering Techniques (Program Summary), Computer Physics Communications, Submitted. [2] Erhan Okuyan, Ugur Güdükbay, and Oguz Gülseren, Pattern Information Extraction from Crystal Structures, Computer Physics Communications, 176 (2007) 486. © 2013 Elsevier B.V.","Basis vectors; Crystal; Crystallography; Material science; Pattern recognition; Primitive vectors; Space group; Symmetry","Amorphous materials; Application programming interfaces (API); Artificial intelligence; Atoms; C++ (programming language); Codes (symbols); Computer graphics; Computer networks; Computer operating systems; Computer software; Crystal atomic structure; Crystal structure; Crystallography; Crystals; Data mining; Information analysis; Information retrieval; Pattern recognition; Personal computers; Problem oriented languages; Program debugging; Program processors; Software testing; Studios; Vector spaces; Vectors; Visualization; Windows operating system; Basis vector; Catalogue identifiers; Computational capability; Development environment; Fractional coordinates; Material science; Performance optimizations; Space Groups; Crystal symmetry","Elsevier B.V."
"Marcos-Abed J.","Using a COAC # for CS1",2014,"Proceedings of WCCCE 2014: The 19th Western Canadian Conference on Computing Education - In-Cooperation with ACM SIGCSE",,,"10","","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905577485&doi=10.1145%2f2597959.2597971&partnerID=40&md5=64ef60fccaebab3348867f968841f8da","One of the topics that should be covered in a CS1 course is Iterative Control Structures. This is an important but difficult topic for novice students who, for the first time, are learning a programming language. To improve the understanding of loops, a tool called COAC # was developed, to show in a graphic and animated form the loop functionality. Together with a practice, COAC # was used with a set of test cases so that the student can understand the result associated with each test case. The key to make this tool successful was its user interface, therefore, special attention was placed on the development of a good user interface design. The first experimental results show that it is a very effective tool for program visualization and an easy way to explain some concepts, like simple loops, strings and arrays, to novice students. © 2014 ACM.","C # programming; Computer languages; CS1; Iterative Control Structures; Loops; Programming fundamentals; Repetition","C++ (programming language); Computer programming languages; User interfaces; C# programming; CS1; Iterative control; Loops; Programming fundamentals; Repetition; Students","Association for Computing Machinery"
"Leetmaa M., Skorodumova N.V.","KMCLib: A general framework for lattice kinetic Monte Carlo (KMC) simulations",2014,"Computer Physics Communications","185","9",,"2340","2349",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902077831&doi=10.1016%2fj.cpc.2014.04.017&partnerID=40&md5=02192b73860605eb10c0748420505452","KMCLib is a general framework for lattice kinetic Monte Carlo (KMC) simulations. The program can handle simulations of the diffusion and reaction of millions of particles in one, two, or three dimensions, and is designed to be easily extended and customized by the user to allow for the development of complex custom KMC models for specific systems without having to modify the core functionality of the program. Analysis modules and on-the-fly elementary step diffusion rate calculations can be implemented as plugins following a well-defined API. The plugin modules are loosely coupled to the core KMCLib program via the Python scripting language. KMCLib is written as a Python module with a backend C++ library. After initial compilation of the backend library KMCLib is used as a Python module; input to the program is given as a Python script executed using a standard Python interpreter. We give a detailed description of the features and implementation of the code and demonstrate its scaling behavior and parallel performance with a simple one-dimensional A-B-C lattice KMC model and a more complex three-dimensional lattice KMC model of oxygen-vacancy diffusion in a fluorite structured metal oxide. KMCLib can keep track of individual particle movements and includes tools for mean square displacement analysis, and is therefore particularly well suited for studying diffusion processes at surfaces and in solids. Program summary Program title: KMCLib Catalogue identifier: AESZ-v1-0 Program summary URL:http://cpc.cs.qub.ac. uk/summaries/AESZ-v1-0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: 49 064 No. of bytes in distributed program, including test data, etc.: 1 575 172 Distribution format: tar.gz Programming language: Python and C++. Computer: Any computer that can run a C++ compiler and a Python interpreter. Operating system: Tested on Ubuntu 12.4 LTS, CentOS release 5.9, Mac OSX 10.5.8 and Mac OSX 10.8.2, but should run on any system that can have a C++ compiler, MPI and a Python interpreter. Has the code been vectorized or parallelized?: Yes. From one to hundreds of processors depending on the type of input and simulation. RAM: From a few megabytes to several gigabytes depending on input parameters and the size of the system to simulate. Classification: 4.13, 16.13. External routines: KMCLib uses an external Mersenne Twister pseudo random number generator that is included in the code. A Python 2.7 interpreter and a standard C++ runtime library are needed to run the serial version of the code. For running the parallel version an MPI implementation is needed, such as e.g. MPICH from http://www.mpich.org or Open-MPI from http://www.open-mpi.org. SWIG (obtainable from http://www.swig.org/) and CMake (obtainable from http://www.cmake.org/) are needed for building the backend module, Sphinx (obtainable from http://sphinx-doc.org) for building the documentation and CPPUNIT (obtainable from http://sourceforge.net/projects/cppunit/) for building the C++ unit tests. Nature of problem: Atomic scale simulation of slowly evolving dynamics is a great challenge in many areas of computational materials science and catalysis. When the rare-events dynamics of interest is orders of magnitude slower than the typical atomic vibrational frequencies a straight-forward propagation of the equations of motions for the particles in the simulation cannot reach time scales of relevance for modeling the slow dynamics. Solution method: KMCLib provides an implementation of the kinetic Monte Carlo (KMC) method that solves the slow dynamics problem by utilizing the separation of time scales between fast vibrational motion and the slowly evolving rare-events dynamics. Only the latter is treated explicitly and the system is simulated as jumping between fully equilibrated local energy minima on the slow-dynamics potential energy surface. Restrictions: KMCLib implements the lattice KMC method and is as such restricted to geometries that can be expressed on a grid in space. Unusual features: KMCLib has been designed to be easily customized, to allow for user-defined functionality and integration with other codes. The user can define her own on-the-fly rate calculator via a Python API, so that site-specific elementary process rates, or rates depending on long-range interactions or complex geometrical features can easily be included. KMCLib also allows for on-the-fly analysis with user-defined analysis modules. KMCLib can keep track of individual particle movements and includes tools for mean square displacement analysis, and is therefore particularly well suited for studying diffusion processes at surfaces and in solids. Additional comments: The full documentation of the program is distributed with the code and can also be found at http://www.github.com/leetmaa/KMCLib/manual Running time: rom a few seconds to several days depending on the type of simulation and input parameters. © 2014 Elsevier B.V. All rights reserved.","Diffusion; Kinetic Monte Carlo; KMC; Python; Simulation framework","Codes (symbols); Diffusion; Equations of motion; Fluorspar; Kinetics; Metals; Monte Carlo methods; Number theory; One dimensional; Open source software; Particle size analysis; Potential energy; Problem oriented languages; Program compilers; Program documentation; Program interpreters; Quantum chemistry; Random number generation; Software testing; Computational materials science; Kinetic Monte Carlo; Kinetic Monte Carlo methods; Lattice kinetic Monte Carlo; Pseudo random number generators; Python; Simulation framework; Three-dimensional lattices; C++ (programming language)","Elsevier B.V."
"Sasvári A., Baharev A.","SG2PS (structural geology to postscript converter) - A graphical solution for brittle structural data evaluation and paleostress calculation",2014,"Computers and Geosciences","66",,,"81","93",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894490683&doi=10.1016%2fj.cageo.2013.12.010&partnerID=40&md5=4d2df882c3f51772d8c441510f1cbacc","The aim of this work was to create an open source cross platform application to process brittle structural geological data with seven paleostress inversion algorithms published by different authors and formerly not available within a single desktop application. The tool facilitates separate processing and plotting of different localities, data types and user made groups, using the same single input file. Simplified data input is supported, requiring as small amount of data as possible. Data rotation to correct for bedding tilting, rotation with paleomagnetic declination and k-means clustering are available. RUP and ANG stress estimators calculation and visualization, resolved shear direction display and Mohr circle stress visualization are available. RGB-colored vector graphical outputs are automatically generated in Encapsulated PostScript and Portable Document Format. Stereographical displays on great circle or pole point plot, equal area or equal angle net and upper or lower hemisphere projections are implemented. Rose plots displaying dip direction or strike, with dip angle distribution of the input data set are available. This tool is ideal for preliminary data interpretation on the field (quick processing and visualization in seconds); the implemented methods can be regularly used in the daily academic and industrial work as well.The authors' goal was to create an open source and self-contained desktop application that does not require any additional third party framework (such as .NET) or the Java Virtual Machine. The software has a clear and highly modular structure enabling good code portability, easy maintainability, reusability and extensibility. A Windows installer is publicly available and the program is also fully functional on Linux. The Mac OS X port should be feasible with minimal effort. The install file with test and demo data sets, detailed manual, and links to the GitHub repositories are available on the regularly updated website www.sg2ps.eu. © 2014 Elsevier Ltd.","Angelier method; Cluster analysis; Graphical user interface; Paleostress inversion; RUP; Slickenside","Cluster analysis; Computer operating systems; Computer software portability; Computer software reusability; Data visualization; Graphical user interfaces; K-means clustering; Open source software; Reusability; Visualization; Angelier method; Automatically generated; Cross platform applications; Encapsulated postscripts; Paleo-stress; Paleomagnetic declinations; Portable document formats; Slickenside; Structural geology; algorithm; brittle deformation; cluster analysis; computer simulation; data inversion; graphical method; numerical model; paleomagnetism; paleostress; slickenside; software; structural geology; visualization","Elsevier Ltd"
"Karaci A., Arici N.","Determining students' level of page viewing in intelligent tutorial systems with artificial neural network",2014,"Neural Computing and Applications","24","3-4",,"675","684",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893956831&doi=10.1007%2fs00521-012-1284-8&partnerID=40&md5=993d60e08f08819233a38060d3509429","The concept of level of page viewing (LPV) refers to the extent to which a student actively revises the pages that he or she has to study in tutorial systems. In the present study, an artificial neural network (ANN) model, which is composed of 5 inputs, 20 and 30 neurons, 2 hidden layers, and 1 output, was designed to determine the students' LPV. After this network was trained, it was integrated into a web-based prototype teaching system, which was developed by ASP.net C# programming language. Additionally, Decision Tree method is tried to determine students' LPV. However, this method gave wrong results according to expected LPV values. In this system, the student first studies the pages uploaded by the teacher onto the system. After studying all the pages within the scope of a topic, the student can go to the test page for evaluation purposes. LPVs of a student who wants to navigate to the test page are calculated by an ANN module added to the system. On the condition that one or more of the LPV's are not up to the desired level, the student is not allowed to take the test and is informed of the pages with missing LPV's so that he can re-study these pages. This prototype system developed based on ANN to determine students' LPV is essential for intelligent tutorial systems, geared to provide intelligent assistance and guidance. The system can track the pages which the students did not study sufficiently and thus direct them to relevant pages. How much activity the students perform on each page to study is observed before they actually take the test, and the areas which should be further revised are determined much in advance. © 2012 Springer-Verlag London.","Artificial neural network; Intelligent tutorial system; Learning management system; Level of page viewing",,
"Frajták K., Bureš M., Jelínek I.","Reducing user input validation code in web applications using Pex extension",2014,"ACM International Conference Proceeding Series","883",,,"302","308",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908680904&doi=10.1145%2f2659532.2659633&partnerID=40&md5=a5fcfc2ab9c362a823fdea18ba780281","Validation of user input data is very important in web application. Not only it protects the system from various exploits, but it also improves the user experience. User immediately sees what values are missing or are not valid and should be fixed. It is important to validate code on client side in the browser, but that does not mean that the validation on server side can be omitted. The golden rule of the web applications is not to trust user input and validate code on server side as well. The user input validation is therefore duplicated - it validates the input values first on client side using JavaScript before the data is sent to server and then the received data is validated again on the server side. Changes made to the validation code must be synchronized in code on both sides. All implementations must be also unit tested, multiple sets of unit tests must be created and maintained. We will describe how we extended white-box testing tool Pex to generate user input validation code for web applications created on .NET platform. The JavaScript client side validation code is generated from the controller code written in C#. The code then validates input values on the client side. Most of the testing can be automated executing generated test. Testing resources - i.e time spent on testing and number of testers involved people - are saved. Copyright © 2014 ACM.","Code generation; User input validation; Web application testing","Automatic test pattern generation; High level languages; Network security; User experience; Code Generation; Input values; Server sides; Testing resources; User input validations; WEB application; Web application testing; White-box testing; Codes (symbols)","Association for Computing Machinery"
"Mohammed-Azizi B., Medjadi D.E.","Single particle calculations for a Woods–Saxon potential with triaxial deformations, and large Cartesian oscillator basis (TRIAXIAL 2014, Third version of the code Triaxial)",2014,"Computer Physics Communications","185","11",,"3067","3068",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026756181&doi=10.1016%2fj.cpc.2014.07.016&partnerID=40&md5=9050b9f783ac7325886c59b04f4dd029","Theory and FORTRAN program of the first version of this code (TRIAXIAL) have already been described in detail in Computer Physics Comm. 156 (2004) 241–282. A second version of this code (TRIAXIAL 2007) has been given in CPC 176 (2007) 634–635. The present FORTRAN program is the third version (TRIAXIAL 2014) of the same code. Now, It is written in free format. As the former versions, this FORTRAN program solves the same Schrodinger equation of the independent particle model of the atomic nucleus with the same method. However, the present version is much more convenient. In effect, it is characterized by the fact that the eigenvalues and the eigenfunctions can be given by specific subroutines. The latters did not exist in the old versions (2004 and 2007). In addition, it is to be noted that in the previous versions, the eigenfunctions were only given by their coefficients of their expansion onto the harmonic oscillator basis. This method is needed in some cases. But in other cases, it is preferable to treat the eigenfunctions directly in configuration space. For this reason, we have implemented an additional subroutine for this task. Some other practical subroutines have also been implemented. Moreover, eigenvalues and eigenfunctions are recorded onto several files. All these new features of the code and some important aspects of its structure are explained in the document ‘Triaxial2014 use.pdf’. New version program summary Program title: Triaxial2014 Catalogue identifier: ADSK_v3_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/ADSK_v3_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 13672 No. of bytes in distributed program, including test data, etc.: 217598 Distribution format: tar.gz Programming language: FORTRAN 77/90 (double precision). Computer: PC. Pentium 4, 2600MHz and beyond. Operating system: WINDOWS XP, WINDOWS 7, LINUX. RAM: 256 Mb (depending on nmax). Swap file: 4Gb (depending on nmax) Classification: 17.7. Does the new version supersede the previous version?: Yes Catalogue identifier of previous version: ADSK_v2_0 Journal reference of previous version: Comput. Phys. Comm. 176 (2007) 634 Nature of problem: The Single particle energies and the single particle wave functions are calculated from one-body Hamiltonian including a central field of Woods–Saxon type, a spin–orbit interaction, and the Coulomb potential for the protons. We consider only ellipsoidal (triaxial) shapes. The deformation of the nuclear shape is fixed by the usual Bohr parameters (β,γ). Solution method: The representative matrix of the Hamiltonian is built by means of the Cartesian basis of the anisotropic harmonic oscillator, and then diagonalized by a set of subroutines of the EISPACK library. Two quadrature methods of Gauss are employed to calculate respectively the integrals of the matrix elements of the Hamiltonian, and the integral defining the Coulomb potential. Two quantum numbers are conserved: the parity and the signature. Due to the Kramers degeneracy, only positive signature is considered. Therefore, calculations are made for positive and negative parity separately (with positive signature only). Reasons for new version: Now, there are several ways to obtain the eigenvalues and the eigenfunctions. The eigenvalues can be obtained from the subroutine ‘eigvals’ or from the array ‘energies’ or also from the formatted files ‘valuu.dat’, ‘eigenvalo.dat’, ‘eigenva.dat’ or better from the unformatted file ‘eigenvaunf.dat’. The eigenfunctions can be obtained straightforwardly in configuration space from the subroutine ‘eigfunc’ or by their components on the oscillator basis from the subroutine ‘compnts’. The latter are also recorded on a formatted file ‘componento.dat’ or on an unformatted file ‘componentounf.dat’. Summary of revisions: This version is characterized by the fact that the eigenvalues and the eigenfunctions can be given by specific subroutines which did not exist in the old versions (2004 and 2007) of the program. Moreover, the eigenvalues and the eigenfunctions can also be deduced directly from files. It is to be noted that this version is now written in free format. All these reasons contribute to make the use of this code easier. Restrictions: There are two restrictions for the code: The number of the major shells of the basis should not exceed Nmax=26 (which is very sufficient in usual cases). For the largest values of Nmax (∼23–26), the diagonalization takes the major part of the running time, but the global run-time remains reasonable. Additional comments: Software used: (1) COMPAC VISUAL FORTRAN (with full optimizations in the settings project options on WINDOWS XP); (2) SILVERFROST PLATO VERSION 4.63 (with debug.net option on WINDOWS 7); (3) APPROXIMATRIX SIMPLY FORTRAN VERSION 2.13 BUILD (on WINDOWS XP and WINDOWS 7). Running time: (With full optimization in the project settings of the Compaq Visual Fortran on Windows XP) With NMAX=23, for the neutrons case, the running time is about 50 s on the intel core i5 processor. © 2014 Elsevier B.V.","Energy levels; Nuclear physics; Schrodinger equation; Wave functions; Woods–Saxon potential","Codes (symbols); Computer operating systems; Coulomb blockade; Deformation; Electric fields; Electron energy levels; FORTRAN (programming language); Hamiltonians; Matrix algebra; Nuclear physics; Oscillators (mechanical); Quantum theory; Schrodinger equation; Software testing; Subroutines; Wave functions; Windows operating system; Catalogue identifiers; Configuration space; Distributed program; Harmonic oscillators; Independent particle models; Single-particle calculations; Single-particle energy; Triaxial deformation; Eigenvalues and eigenfunctions","Elsevier B.V."
"Okuyan E., Okuyan E.","A tool for pattern information extraction and defect quantification from crystal structures",2015,"Computer Physics Communications","187",,,"266","267",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919628742&doi=10.1016%2fj.cpc.2014.09.017&partnerID=40&md5=93a39ec19a36b6bc3d412f327878168e","In this paper, we present a revised version of BilKristal 2.0 tool. We added defect quantification functionality to assess crystalline defects. We improved visualization capabilities by adding transparency support and runtime visibility sorting. Discovered bugs are fixed and small performance optimizations are made. New version program summary Program title: BilKristal 3.0 Catalogue identifier: ADYU-v3-0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/ADYU-v3-0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 1868 923 No. of bytes in distributed program, including test data, etc.: 8854 507 Distribution format: tar.gz Programming language: C, C++, Microsoft.NET Framework 2.0 and OpenGL Libraries. Computer: Personal Computers with Windows operating system. Operating system: Windows XP or higher. RAM: 20-60 Megabytes. Classification: 8. Catalogue identifier of previous version: ADYU-v2-0 Journal reference of previous version: Comput. Phys. Comm. 185 (2014) 442 External routines: Microsoft.NET Framework 2.0. For the visualization tool, graphics card driver should also support OpenGL. Does the new version supersede the previous version?: Yes Nature of problem: Determining the crystal structure parameters of a material is a very important issue in crystallography. Knowing the crystal structure parameters helps the understanding of the physical behavior of material. For complex structures, particularly for materials which also contain local symmetry as well as global symmetry, obtaining crystal parameters can be very hard. Solution method: The tool extracts crystal parameters such as primitive vectors, basis vectors and identifies the space group from atomic coordinates of crystal structures. Reasons for new version: Additional features, Compatibility issues with newer development environments, Performance optimizations, Minor bug corrections. Summary of revisions:Defect quantification capability is added. The tool can process the imperfect crystal structures, finds and quantifies the crystalline defects. The tool is capable of finding positional defects, vacancy defects, substitutional impurities and interstitial impurities. The algorithms presented in [3] are used for defect quantification implementation.Transparency support is added to the visualization tool. Users are now allowed to set the transparency of each atom type individually.Runtime visibility sorting functionality is added to facilitate correct transparency computations.Visual Studio 2012 support is added. Visual Studio 2012 specific project files are created and the project is tested with this development environment.In visualization tool, an unused log file was created. This issue is corrected.In visualization tool, some OpenGL calls which are executed at every draw are changed to be executed only when they are needed, improving the visualization performance.Restrictions: Assumptions are explained in [1,2]. However, none of them can be considered as a restriction onto the complexity of the problem. Running time: The tool was able to process input files with more than a million atoms in less than 20 s on a PC with an Athlon quad-core CPU at 3.2 GHz using the default parameter values. References: [1] Erhan Okuyan, Ugur Güdükbay, Oguz Gülseren, Pattern information extraction from crystal structures, Comput. Phys. Comm. 176 (2007) 486. [2] Erhan Okuyan, Ugur Güdükbay, BilKristal 2.0: A tool for pattern information extraction from crystal structures, Comput. Phys. Comm. 185 (2014) 442. [3] Erhan Okuyan, Ugur Güdükbay, Ceyhun Bulutay, Karl-Heinz Heinig, MaterialVis: material visualization tool using direct volume and surface rendering techniques, J. Mol. Graphics Model. 50201450-60. © 2014 The Authors.","Basis vectors; Crystallography; Defect quantification; Material science; Pattern recognition; Primitive vectors; Space group; Symmetry","Application programming interfaces (API); Atoms; C++ (programming language); Computer graphics; Computer networks; Crystal atomic structure; Crystal impurities; Crystallography; Information retrieval; Nanocrystalline materials; Pattern recognition; Personal computers; Problem oriented languages; Software testing; Sorting; Studios; Substitution reactions; Transparency; Vector spaces; Vectors; Visibility; Visualization; Windows operating system; Basis vector; Catalogue identifiers; Development environment; Interstitial impurities; Material science; Performance optimizations; Space Groups; Substitutional impurities; Crystal symmetry","Elsevier B.V."
"Cseppento L., Micskei Z.","Evaluating symbolic execution-based test tools",2015,"2015 IEEE 8th International Conference on Software Testing, Verification and Validation, ICST 2015 - Proceedings",,,"7102587","","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935117025&doi=10.1109%2fICST.2015.7102587&partnerID=40&md5=dcbab6b72f1346ea85b95ba812c8d2ff","In recent years several symbolic execution-based tools have been developed to automatically select relevant test inputs from the source code of the system under test. However, each of these tools has different advantages, and there is no detailed feedback available on the actual capabilities of the various tools. In order to evaluate test input generators we collected a representative set of programming language concepts that should be handled by the tools, mapped them to 300 code snippets that would serve as inputs for the tools, created an automated framework to execute and evaluate these snippets, and performed experiments on four Java and one.NET test generator tools. The results highlight the strengths and weaknesses of each tool, and identify hard code parts that are difficult to tackle for most of the tools. We hope that our research could serve as actionable feedback to tool developers and help practitioners assess the readiness of test input generation. © 2015 IEEE.",,"Model checking; Testing; Detailed feedbacks; Generator tool; Source codes; Symbolic execution; System under test; Test inputs; Test tools; Software testing","Institute of Electrical and Electronics Engineers Inc."
"Phan A.-D., Hansen M.R.","An approach to multicore parallelism using functional programming: A case study based on Presburger Arithmetic",2015,"Journal of Logical and Algebraic Methods in Programming","84","1","10","2","18",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938704222&doi=10.1016%2fj.jlamp.2014.07.002&partnerID=40&md5=01740daf97bb65f5a12b9b9e41a5e082","Abstract In this paper we investigate multicore parallelism in the context of functional programming by means of two quantifier-elimination procedures for Presburger Arithmetic: one is based on Cooper's algorithm and the other is based on the Omega Test. We first develop correct-by-construction prototype implementations in a functional programming language. Thereafter, the parallelism inherent in the decision procedures is analyzed using the Directed Acyclic Graph (DAG) model of multicore parallelism. In the step from a DAG model to a parallel implementation, the parallel implementation is optimized taking into account negative factors such as cache misses, garbage collection and overhead due to task creations, because such factors may introduce sequential bottlenecks with severe consequences for the parallel efficiency. The experiments were conducted using the functional programming language F# and.NET platform executing on an 8-core machine. A speedup of approximately 4 was obtained for Cooper's algorithm and a speedup of approximately 6 was obtained for the exact-shadow part of the Omega Test. The considered procedures are complex, memory-intense algorithms on huge formula trees and the case study reveals more general applicable techniques and guideline for deriving parallel algorithms from sequential ones in the context of data-intensive tree algorithms. The obtained insights should apply for any strict and impure functional programming language. Furthermore, the results obtained for the exact-shadow elimination procedure have a wider applicability because they can directly be transferred to the Fourier-Motzkin elimination method. © 2014 Elsevier Inc.","Decision procedure; Functional programming; Multicore; Parallelism; Presburger Arithmetic","Ada (programming language); Algorithms; Computational linguistics; Computer programming languages; Digital arithmetic; Directed graphs; Forestry; Multicore programming; Software prototyping; Trees (mathematics); Decision procedure; Directed acyclic graph (DAG); Fourier-motzkin elimination; Multi core; Parallel implementations; Parallelism; Presburger arithmetic; Prototype implementations; Functional programming; Algorithms; Decision Making; Languages","Elsevier Inc."
"Wang Y.","The application of computer information processing technology in the design of school examination system",2015,"International Journal of Simulation: Systems, Science and Technology","16","5B",,"14.1","14.7",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992390933&doi=10.5013%2fIJSSST.a.16.5B.14&partnerID=40&md5=9a14e819810e00c4e2952cbab55300f9","With the development of computer technology, more and more schools and training institutions in the society have begun to use the network teaching. In the school can use the basis of campus network, network teaching platform. The erection of other teaching test platform can use their own or lease the server operators to. With the continuous innovation and the accelerating pace of society and technology, more and more people need to use their free time learning, which gave birth to the emergence of network education platform. In schools in order to facilitate teaching, combining the education and modern network technology to better, many colleges and universities have their own system of examination system, which is a college should possess. Developed a set of test system using Microsoft Corp's ASP.NET technology, network examination can be carried on a number of subjects, the examination time is more free, saving a lot of manpower and material resources, reduce the work of teachers, improve the efficiency of the work, can be more fair and impartial examination. The system uses the browser mode or B/S mode to realize the automatic test, automatic marking, the establishment of database etc.. The system adopts B/S three layer structure mode, reduce the workload of system maintenance, can log on to the system, convenient for the teachers to establish database, the database is updated. Background using Microsoft Corp's SQL Server 2005 as the database, the interface shows the use of ASP.NET technology, both of which can be very good together, convenient data query and change. The system realizes the online automatic extraction, automatic marking, the examination question bank management, achievement upload, query results, and the results of the modification and other functions, can be convenient to test a series of network automation, realize the paperless examination, and has wide popularization prospect. © 2015, UK Simulation Society. All rights reserved.","ASP technology; B/S; Examination system; The database","Database systems; Education; Query processing; Societies and institutions; Teaching; Windows operating system; ASP technology; Colleges and universities; Examination system; Information processing technology; Network education platforms; Network teaching platforms; Society and technologies; Three-layer structures; Engineering education","UK Simulation Society"
"Leetmaa M., Skorodumova N.V.","KMCLib 1.1: Extended random number support and technical updates to the KMCLib general framework for kinetic Monte-Carlo simulations",2015,"Computer Physics Communications","196",,,"611","613",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942111540&doi=10.1016%2fj.cpc.2015.06.016&partnerID=40&md5=70547adde63052334fc6aef7c257af2d","We here present a revised version, v1.1, of the KMCLib general framework for kinetic Monte-Carlo (KMC) simulations. The generation of random numbers in KMCLib now relies on the C++11 standard library implementation, and support has been added for the user to choose from a set of C++11 implemented random number generators. The Mersenne-twister, the 24 and 48 bit RANLUX and a 'minimal-standard' PRNG are supported. We have also included the possibility to use true random numbers via the C++11 std::random-device generator. This release also includes technical updates to support the use of an extended range of operating systems and compilers. New version program summary Program title: KMCLib v1.1 Catalogue identifier: AESZ-v1-1 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AESZ-v1-1.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: 49,398 No. of bytes in distributed program, including test data, etc.: 1,536,855 Distribution format: tar.gz Programming language: Python and C++. Computer: Any computer that can run a C++11 compatible C++ compiler and a Python 2.7 interpreter. Operating system: Tested on Ubuntu 14.4 LTS, Ubuntu 12.4 LTS, CentOS 6.6, Mac OSX 10.10.3, Mac OSX 10.9.5 and Mac OSX 10.8.2 but should run on any system that can have a C++11 compatible C++ compiler and a Python 2.7 interpreter. Has the code been vectorized or parallelized?: Yes, with MPI. From one to hundreds of processors may be used depending on the type of input and simulation. RAM: From a few megabytes to several gigabytes depending on input parameters and the size of the system to simulate. Catalogue identifier of previous version: AESZ-v1-0 Journal reference of previous version: Comput. Phys. Comm. 185 (2014) 2340 Classification: 4.13, 16.13. External routines: To run the serial version of KMCLib no external libraries are needed other than the standard C++ runtime library and a Python 2.7 interpreter with support for numpy. For running the parallel version an MPI implementation is needed, such as e.g. MPICH from http://www.mpich.org or Open-MPI from http://www.open-mpi.org. SWIG (obtainable from http://www.swig.org/) and CMake (obtainable from http://www.cmake.org/) are both needed for building the backend module, while Sphinx (obtainable from http://sphinx-doc.org) is needed for building the documentation. CPPUNIT (obtainable from http://sourceforge.net/projects/cppunit/, also included in the KMCLib distribution) is needed for building the C++ unit tests Does the new version supersede the previous version?: Yes Nature of problem: Atomic scale simulation of slowly evolving dynamics is a great challenge in many areas of computational materials science and catalysis. When the rare-events dynamics of interest is orders of magnitude slower than the typical atomic vibrational frequencies a straight-forward propagation of the equations of motions for the particles in the simulation cannot reach time scales of relevance for modeling the slow dynamics. Solution method: KMCLib provides an implementation of the kinetic Monte Carlo (KMC) method that solves the slow dynamics problem by utilizing the separation of time scales between fast vibrational motion and the slowly evolving rare-events dynamics. Only the latter is treated explicitly and the system is simulated as jumping between fully equilibrated local energy minima on the slow-dynamics potential energy surface. Reasons for new version: The v1.1 revision increases the reliability and flexibility of the random number generation options in KMCLib, which is a central part of the KMC algorithm. The new release also comes with extended support for additional compilers and updates to the build system to simplify the installation procedure on some widely used platforms. Summary of revisions:Enough time has passed since the introduction of the <random> header in the C++ standard runtime library with the C++11 standard, that most installed compilers today have support to enable the use of C++11 specific language features in C+++. The <random> standard header comes with a set of well-defined pseudo random number generators (PRNG). Using standard library routines in favor of custom implementations has the obvious advantage of being more reliable and with guaranteed support over a longer time. From the v1.1 revision, KMCLib therefore relies on the C++11 standard library <random> header to produce pseudo-random numbers. This also makes it easier to enable support for several different PRNG:s for the user to choose from. From previously only supporting a Mersenne-twister implementation, KMCLib now has support for using the Mersenne-twister [1], the 24 and 48-bit RANLUX [2] generators, as well as a 'minimal-standard' PRNG [3].For machines with a random device installed, KMCLib v1.1 can run simulations with true random numbers. This is enabled by using the std::random-device generator in C++. If the random device is properly installed the true random numbers are available to KMCLib out of the box and the user only needs to specify the use of the random device with an input flag in the same way as she chooses any of the available PRNG:s.The v1.1 revision includes major updates to the build system. The build system has no effect on the outcome of the simulations, but has a great impact on how easy it is to install the program. The Intel compiler is widely available on super computer clusters and support for this compiler widely extends the number of systems where KMCLib can be easily setup and run. The popularity of the Mac platform also makes smooth installation and compilation with clang desirable. With version v1.1 the make system for KMClib now includes support for the clang compiler on Mac and support for both the Intel compiler and the gcc compiler on Linux. See the reference manual for details of which versions of the operating systems and compilers have been tested.Restrictions: KMCLib implements the lattice KMC method and is as such, restricted to geometries that can be expressed on a grid in space. See the original paper describing KMCLib [4] for further details. Unusual features: KMCLib has been designed to be easily customized, to allow for user-defined functionality and integration with other codes. The user can define her own on-the-fly rate calculator via a Python API, so that site-specific elementary process rates, or rates depending on long-range interactions or complex geometrical features can easily be included. KMCLib also allows for on-the-fly analysis with user-defined analysis modules. KMCLib can keep track of individual particle movements and includes tools for mean square displacement analysis based on the algorithm described in Ref. [5], and is therefore particularly well suited for studying diffusion processes at surfaces and in solids. With the release of v1.1 KMCLib now supports several different pseudo random number generators, but can also, if a random device is installed on the machine, use true random numbers via the std::random-device generator. Additional comments: The full documentation of the program is distributed with the code and can also be found online at http://leetmaa.github.io/KMCLib/manual-v1.1/. Running time: From a few seconds to several days depending on the type of simulation and input parameters. References:M. Matsumoto and T. Nishimura, ""Mersenne Twister: A 623- dimensionally equidistributed uniform pseudorandom number generator"", ACM Trans. on Modeling and Computer Simulation, 8 (1998) 3.M. Lscher, ""A portable high-quality random number generator for lattice field theory calculations"", Computer Physics Communications, 79 (1994) 100110.S. K. Park, K. W. Miller and P K. Stockmeyer, ""Technical correspondence"", Communications of the ACM, 36 (1993) 105.M. Leetmaa and N. V. Skorodumova, ""KMCLib: A general framework for lattice kinetic Monte Carlo (KMC) simulations"", Computer Physics Communications, 185 (2014) 2340.M. Leetmaa and N. V. Skorodumova, ""Mean square displacements with error estimates from non-equidistant time-step kinetic Monte Carlo simulations"", Computer Physics Communications, 191 (2015) 119. © 2015 Elsevier B.V.","Diffusion; Kinetic; KMC; Monte Carlo; PRNG; Random device; Simulation; True random numbers","C++ (programming language); Codes (symbols); Computation theory; Diffusion; Equations of motion; Kinetics; Lattice theory; Linux; Monte Carlo methods; Number theory; Open source software; Particle size analysis; Potential energy; Problem oriented languages; Program compilers; Program documentation; Quantum chemistry; Software testing; Computational materials science; Kinetic monte carlo simulation; Lattice kinetic Monte Carlo; PRNG; Pseudo random number generators; Random device; Simulation; True randoms; Random number generation","Elsevier B.V."
"Sobuś J., Woda M.","CPU utilization analysis of selected genetic algorithms in multi-core systems for a certain class of problems",2016,"Advances in Intelligent Systems and Computing","470",,,"431","444",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976367286&doi=10.1007%2f978-3-319-39639-2_38&partnerID=40&md5=fe98b966db42385b8b40da1446c78fea","This work was carried out in order to examine and compare selected models of genetic algorithms (through the implementation), using the latest tools and libraries that allow for multithreaded programming in a.NET environment. Implemented algorithms were then tested for the use of available resources, such as CPU cycles/cores consumption and the time at which they are able to provide the quality results at acceptable pace. With a choice of multi-core processors—allowing for parallel calculations on their cores, as well as genetic algorithms, one should think about how to implement the chosen algorithm so as to avoid the deadlocks and bottlenecks to make optimal use of the computing power of cores. There are many approaches to deal with such issues—a lot of tools and software libraries facilitate the implementation of such algorithms. This paper tries to address two essential questions what algorithms fit the best into multicore architecture, and which one benefits the best from available logical/physical cores producing the best possible results. © Springer International Publishing Switzerland 2016.","Genetic algorithms; Implementation; Multi-threaded computation; Resources consumption","Genetic algorithms; Multicore programming; Software architecture; Implementation; Multi-core processor; Multicore architectures; Multithreaded; Multithreaded programming; Parallel calculation; Resources consumption; Software libraries; Algorithms","Springer Verlag"
"Rori C., Munir R., Paseru D., Manembu P.","Fan temperature detection using microcontroller",2016,"Proceeding of 2015 1st International Conference on Wireless and Telematics, ICWT 2015",,,"7449223","","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969135974&doi=10.1109%2fICWT.2015.7449223&partnerID=40&md5=df498def3173e0609174e67640e82b1d","Current fan is not able to display room temperature and humidity. It also cannot run automatically on a specified time or room temperature. Therefore it should consider a system that can overcome these weaknesses. This research will make a room temperature control system that can detect temperature and humidity, and displays the results to the application. Systems made useful to control the fan so that the room temperature is still cool. The system will interact with the user through the application, both for switching on and off the fan directly or automatically based on temperature and time. This research uses the C# programming language in the creation of applications and C for coding of microcontroller. The methodology used is prototyping. Testing conducted concluded that the application can display the values of temperature and humidity, a function to turn on and turn off the fan directly or based on temperature and time can run well. © 2015 IEEE.","detection; fan; microcontroller; temperature","Error detection; Fans; Humidity control; Microcontrollers; Temperature; Wireless telecommunication systems; C# programming; Room-temperature control; Temperature and humidities; Temperature detection; Turn offs; Controllers","Institute of Electrical and Electronics Engineers Inc."
"Varouqa M., Hammo B.","WIT: Weka interface translator",2016,"International Journal of Speech Technology","19","2",,"359","371",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930607876&doi=10.1007%2fs10772-015-9283-7&partnerID=40&md5=e7b8c621673162f69fe625189a41641a","Advancement in technology turns the big world into one small village. Regardless of what country you are living in, what language you are speaking or understanding, you should be able to benefit from the accumulated knowledge available on the Internet. Unfortunately, this is not the case with English being the de facto language of most programming languages, services, tools and web content. Many users are blocked from using these tools and services because they do not speak or understand English. Multilingual software evolved as a solution to this dilemma. In this paper, we describe the design and implementation of a user-friendly toolkit named Weka interface translator (WIT). It is dedicated to internationalize Weka, which is a collection of machine learning algorithms for data mining tasks widely used by many researchers around the world. WIT is a collaboration project between the Arabic natural language processing team from the University of Jordan and Weka’s development team from the University of Waikato. Its main goal is to facilitate the translation process of Weka’s interfaces into multi-languages. WIT is downloadable through SourceForge.net and is officially listed on Weka’s wiki spaces among its related projects. To experiment with WIT, we present Arabic as a pilot test among many languages that could benefit from this project. © 2015, Springer Science+Business Media New York.","Interface translator; Multilingualism; Software globalization; Software internationalization and localization; Weka internationalization","Artificial intelligence; Computational linguistics; Data mining; Learning algorithms; Learning systems; Natural language processing systems; Object oriented programming; Arabic natural language processing; Collaboration projects; Data mining tasks; Design and implementations; Multilingualism; Software globalizations; Translation process; Weka internationalization; Translation (languages)","Springer New York LLC"
"Pescarin S., Cerato I., Romi P.","Virtual museums and social networks",2016,"2016 IEEE 2nd International Forum on Research and Technologies for Society and Industry Leveraging a Better Tomorrow, RTSI 2016",,,"7740551","","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006004202&doi=10.1109%2fRTSI.2016.7740551&partnerID=40&md5=e8e7d88e985b7ec36b76d58dbf6e0be0","In the last decade, virtual museums emerged, becoming a continuous trend for museums that want to innovate, renew and involve their visitors. The European Network of Virtual Museums (V-MUST.NET) in its four years analysis (2011-2014) indicated and tested potentially interesting digital applications, for the future of museums. How these involving narrative applications should be treated and how they should be communicated? From the report recently presented by Civita about museum and social media, emerged that 9 million of people in Italy use social networks to get informed about art and culture. Therefore it seems an important direction to follow for public institutions and museums, to inform people about their activities. What museums and other institutions are currently doing in this direction? How can they fully benefit from social media? Virtual museums can be considered as museums and use the same strategies? In order to answer to those questions, we have started to analyse the interaction between Virtual Museums and institutions, starting with those institutions that participated to the Museum Week (#MuseumWeek). We have, at the same time, promoted a Virtual Museum (The Virtual Museum of the Tiber Valley) during the Museum Week 2016. The results of these activities and tests are presented. © 2016 IEEE.","Collaboration; Communication; Participation; Social media; Virtual Museums","Communication; Social networking (online); Societies and institutions; Collaboration; Digital applications; European networks; Participation; Public institution; Social media; Use social networks; Virtual museum; Museums","Institute of Electrical and Electronics Engineers Inc."
"Miscevich J., Burke L., Grubbs E.","virtual reality and motion controlled data analysis",2017,"Proceedings of the International Telemetering Conference",,,,"","",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041480747&partnerID=40&md5=bd0890c1acaaef2e17c9c42991c2abb1","In this technologically advanced society, statistics and data analytics are becoming increasingly important. As technology progresses, so should our methodologies of accessing and analyzing large data sets. By intergrating the emerging field of virtual reality with data analysis, we can view and better understand data in a more engaging, hands on, and exciting way. Our project seeks to incorporate motion tracking technologies to truly immerse the user in the information that they are exploring. Using an oculus Rift virtual reality headset, a Microsoft kinect motion tracking sensor, and Unity Engine game development suite (using C#), we were able to make data representations that could be modifield via physical motion. By giving users a firsthnd view of their data in a virtual environment, and allowing them to manipulate graphical representations via intutive physical gestures, they are more engagged and more willing and better able to understand the information that they are seeing. This new technique could be very valuable in allowing analysists to view telemetry data from test vehicles in a way to improve efficiency i finding the correct data files and then viewing the actual data.",,"Data handling; Information analysis; Telemetering; Virtual reality; Data representations; Graphical representations; Microsoft kinect; Motion tracking; Motion tracking technology; Physical motion; Technology progress; Virtual-reality headsets; Motion analysis","International Foundation for Telemetering"
"Cseppentő L., Micskei Z.","Evaluating code-based test input generator tools",2017,"Software Testing Verification and Reliability","27","6","e1627","","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012876215&doi=10.1002%2fstvr.1627&partnerID=40&md5=7de8f8e39c2de355c4c027a88d49eebc","In recent years, several tools have been developed to automatically select test inputs from the code of the system under test. However, each of these tools has different advantages, and there is a little detailed feedback available on the actual capabilities of the various tools. To evaluate test input generators, this paper collects a set of programming language concepts that should be handled by the tools and maps these core concepts and challenging features like handling the environment or multi-threading to 363 code snippets, respectively. These snippets would serve as inputs for the tools. Next, the paper presents SETTE, an automated framework to execute and evaluate these snippets. Using SETTE, multiple experiments were performed on five Java and one.NET-based tools using symbolic execution, search-based, and random techniques. The test suites' coverage, size, generation time, and mutation score were compared. The results highlight the strengths and weaknesses of each tool and approach and identify hard code parts that are difficult to tackle for most of the tools. We hope that this research could serve as actionable feedback to tool developers and help practitioners assess the readiness of test input generation. Copyright © 2017 John Wiley & Sons, Ltd.","software testing; test data; test generation; white-box testings","Codes (symbols); Testing; Detailed feedbacks; Generation time; Multi-threading; Symbolic execution; System under test; Test data; Test generations; White box; Software testing","John Wiley and Sons Ltd"
"Taufik I., Syaripudin U., Kaffah F.M., Ismail N., Sobirin J.G., Gunawan T.S.","Concealment of files blocked by gmail with EOF-based image steganography",2018,"Indonesian Journal of Electrical Engineering and Computer Science","12","2",,"716","721",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051827180&doi=10.11591%2fijeecs.v12.i2.pp716-721&partnerID=40&md5=9d47a31f944d1669babaa7072bbac06c","Nowadays, due to security concern, not all the process of sending files via email runs smoothly. There are several types of file extensions that are blocked when sent via email. For examples, there are several file extensions blocked by Gmail. This paper discusses steganographic implementation using End of File (EOF) algorithm to insert special file into image cover file with JPG and PNG format so that files with these extensions can be sent via email. Before a special extension file is inserted into the cover file, a compression process should be conducted first to make the file size smaller. The proposed algorithm is implemented on Visual Basic.Net software. Based on the tests performed, the application can insert Gmail-blocked file system to the image cover file, without changing the physical bit of the image cover file or file system that inserted with 100% success rate. The stego-image file is also successfully sent via email without being blocked. © 2018 Institute of Advanced Engineering and Science. All rights reserved.","End of file; Gmail; Image cover file; Insertion; Steganography",,"Institute of Advanced Engineering and Science"
"Manapure P., Chandak M.","Detection of car-following behavior from malicious environment",2019,"Journal of Advanced Research in Dynamical and Control Systems","11","2 Special Issue",,"2103","2110",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073617587&partnerID=40&md5=75cc4b54aff93fc9ec0ab5512539083a","The Research calibration process is a basic condition of traffic model which uses car following behaviors for road traffic studies. The transmission channel between moving vehicles in real time traffic environment is modeled in road traffic and neighboring environment of connected vehicles adjacent to the road. The choice of input parameters used in the calibration process reflects the success of the car calibration process itself; therefore, the main goal is to choose parameters which give a large influence on the modeling process. Connectivity to the road will provide information to drivers and vehicles to enhance decision making reliability at the operational level. This e-Road project achieves the main goal to enhance safety and efficiency in intelligent transportation systems. The decision to rebroadcast the message is also affected by the situation of the receiver, such as the distance to the original sender, speed, traffic density, and the interest table of neighboring vehicles i.e. history table. To ensure connectivity in a real-time traffic environment, the relation between communication range and vehicles should be considered. This paper describes a detailed analysis of car-following input parameters and its influence on the modeled traveling time. All these findings provide a novel study of traffic flow theory and traffic simulation and used to detect intelligent traffic system. It detects the malicious behavior of various vehicles. The algorithm was implemented in C# dot net and tested under Windows system. © 2019, Institute of Advanced Scientific Research, Inc. All rights reserved.","Calibration process; Car-following behaviors; Connected vehicles; Input parameters; Malicious vehicle",,"Institute of Advanced Scientific Research, Inc."
"Roehm T., Veihelmann D., Wagner S., Juergens E.","Evaluating Maintainability Prejudices with a Large-Scale Study of Open-Source Projects",2019,"Lecture Notes in Business Information Processing","338",,,"151","171",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059943300&doi=10.1007%2f978-3-030-05767-1_10&partnerID=40&md5=5a31f4db46e9706ddba25ed5601ab656","In software engineering, relying on experience can render maintainability expertise into prejudice over time. For example, based on their own experience, some consider JavaScript as inelegant language and hence of lowest maintainability. Such prejudice should not guide decisions without prior empirical validation. Hence, we formulated 10 hypotheses about maintainability based on prejudices and test them in a large set of open-source projects (6,897 GitHub repositories, 402 million lines, 5 programming languages). We operationalize maintainability with five static analysis metrics. We found that JavaScript code is not worse than other code, Java code shows higher maintainability than C# code and C code has longer methods than other code. The quality of interface documentation is better in Java code than in other code. Code developed by teams is not of higher and large code bases not of lower maintainability. Projects with high maintainability are not more popular or more often forked. Overall, most hypotheses are not supported by open-source data. © 2019, Springer Nature Switzerland AG.","Case study; Empirical study; GitHub; Maintainability; Metrics; Open source; Programming language; Software quality; Static analysis","Codes (symbols); Computer programming languages; Computer software selection and evaluation; High level languages; Maintainability; Open source software; Open systems; Quality control; Empirical studies; GitHub; Metrics; Open sources; Software Quality; Static analysis","Springer Verlag"
"Jackson D., Clynch G.","An investigation of the impact of language runtime on the performance and cost of serverless functions",2019,"Proceedings - 11th IEEE/ACM International Conference on Utility and Cloud Computing Companion, UCC Companion 2018",,,"8605773","154","160",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061785641&doi=10.1109%2fUCC-Companion.2018.00050&partnerID=40&md5=234f0e1cc5c4110fc455d7c8b587b59e","Serverless, otherwise known as 'Function-As-A-Service' (FaaS), is a compelling evolution of cloud computing that is highly scalable and event-driven. Serverless applications are composed of multiple independent functions, each of which can be implemented in a range of programming languages. This paper seeks to understand the impact of the choice of language runtime on the performance and subsequent cost of serverless function execution. It presents the design and implementation of a new serverless performance testing framework created to analyse performance and cost metrics for both AWS Lambda and Azure Functions. For optimum performance and cost management of serverless applications, Python is the clear choice on AWS Lambda. C#.NET is the top performer and most economical option for Azure Functions. NodeJS on Azure Functions and.NET Core 2 on AWS should be avoided or at the very least, used carefully in order to avoid their potentially slow and costly start-up times. © 2018 IEEE.","Aws; Azure; Cloud; FaaS; Functions; Lambda; Performance; Serverless","Clouds; Computer programming; Computer science; Functions; Azure; FaaS; Lambda; Performance; Serverless; Cloud computing","Institute of Electrical and Electronics Engineers Inc."
"Fakhruddin H., Kiani K.N.","Hybrid automated test generation tool (HATG)",2019,"2019 International Conference on Information Science and Communication Technology, ICISCT 2019",,,"8777436","","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070686007&doi=10.1109%2fCISCT.2019.8777436&partnerID=40&md5=ed23abb8cf2e87e0b9172be4626484d9","Testing is the primary mode to ensure that the software is defect free and demonstrate high level of reliability. Testing constitutes the critical aspect of software development process which also considered being contributory in realizing the quality goals along with software quality assurance function. We have two types of basic testing techniques; Static and dynamic testing that are used for detecting defects in software product. It is challenging to automate testing phase of the software product which includes both type of testing in a single solution according to the relation they have to each other. We have number of tools available for automated static testing (mainly unit testing) which depends upon random testing inputs independent of external interfaces of the method under test. We also have dynamic testing tool which take inputs from user and apply to the system and matches with user entered output to check whether system has quality output.We have developed a tool named 'Hybrid automated test generation tool that produces test suite automatically with high code coverage. It performs dynamic program analysis using dynamic emblematic execution in order to identify test inputs for created unit tests. By using execution path traces, it learns the behavior of program. HATG uses constraint solver for generating new inputs for different program behavior. We have applied HATG to a server based application written in.NET and found multiple errors including serious issues.We used both type of testing techniques, static and dynamic testing, so that it can create concrete number of inputs with highly coverage area. Our results show that it is more effective for applications that are developed for backend processing and which should be capable of handling any types of data for processing. © 2019 IEEE.","Automated testing; Dynamic Testing; HATG; Software reliability; Software Testing; Static Testing","Automation; Computer software selection and evaluation; Data handling; Defects; Dynamic analysis; Software design; Software reliability; Testing; Automated test generations; Automated testing; Dynamic testing; HATG; Server-based applications; Software development process; Software quality assurance; Static testing; Software testing","Institute of Electrical and Electronics Engineers Inc."
"Islam J.F., Mondal M., Roy C.K.","A Comparative Study of Software Bugs in Micro-clones and Regular Code Clones",2019,"SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering",,,"8667993","73","83",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064164947&doi=10.1109%2fSANER.2019.8667993&partnerID=40&md5=a249ec033237e66e5e99529e6b41030f","Reusing a code fragment through copy/pasting, also known as code cloning, is a common practice during software development and maintenance. Most of the existing studies on code clones ignore micro-clones where the size of a micro-clone fragment can be 1 to 4 LOC. In this paper we compare the bug-proneness of micro-clones with that of regular code clones. From thousands of revisions of six diverse open-source subject systems written in three languages (C, C#, and Java), we identify and investigate both regular and micro-clones that are associated with reported bugs.Our experiment reveals that percentage of changed code fragments due to bug-fix commits is significantly higher in micro-clones than regular clones. The number of consistent changes due to bug-fix commits is significantly higher in micro-clones than regular clones. We also observe that significantly higher percentage of files get affected by bug-fix commits in micro-clones than regular clones. Finally, we found that percentage of severe bugs is significantly higher in micro-clones than regular clones. We perform Mann-Whitney-Wilcoxon (MWW) test to evaluate the statistical significance level of our experimental results. Our findings imply that micro-clones should be emphasized during clone management and software maintenance. © 2019 IEEE.","Code Clones; Micro-Clones; Software Bugs","Codes (symbols); Computer software maintenance; Open source software; Open systems; Program debugging; Reengineering; Software design; Clone management; Code clone; Code fragments; Comparative studies; Open sources; Software bug; Software development and maintenances; Statistical significance; Cloning","Institute of Electrical and Electronics Engineers Inc."
"Morovati V., Dargazany R.","NET v1.0: A framework to simulate permanent damage in elastomers under quasi-static deformations",2019,"SoftwareX","10",,"100229","","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065389565&doi=10.1016%2fj.softx.2019.04.001&partnerID=40&md5=9a4eaa6f140893a63ee88b12fce1ea5b","An analytical software for network evolution theory (NET)to describe the macroscopic behavior of filled rubber-like materials at different loading directions is presented. The software is based on a modular platform that is particularly designed to capture Mullins effect, permanent set, and deformation-induced anisotropy. Other inelastic features of elastomers can be modeled and added to the framework. Only the basic framework is covered in the NET v1.0, which is based on the decomposition of the network to two parallel networks of pure rubber (CC)and a polymer-filler (PP). NET v1.0 is developed in Maple©, has 7 material parameters that should be obtained by fitting, and 21 state variables that will be set internally. Experimental data can automatically be imported into NET from an Excel spreadsheet or direct output of a testing machine. To fit the experimental data, the model will be fitted to one full cycle by using the general Levenberg–Marquardt algorithm. The code helps users to fit and optimize the response of an elastomeric component in conceptual and preliminary design before deciding on implementing in the large-scale FE analysis. © 2019","Deformation-induced anisotropy; Mullins effect; Network evolution theory; Permanent set","Anisotropy; Deformation; Filled polymers; Spreadsheets; Testing; Analytical software; Deformation induced anisotropy; Elastomeric components; Macroscopic behaviors; Mullins effect; Network evolution; Permanent set; Quasi-static deformation; Rubber","Elsevier B.V."
"Uzunbayir S., Kurtel K.","An Analysis on Mutation Testing Tools for C# Programming Language",2019,"UBMK 2019 - Proceedings, 4th International Conference on Computer Science and Engineering",,,"8907222","439","443",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076207183&doi=10.1109%2fUBMK.2019.8907222&partnerID=40&md5=42458cf203052f2880f27384c8cd042f","Mutation testing is a fault-based white-box software testing technique which uses artificial defects known as mutants to represent faulty versions of the application to evaluate the quality of the test suite. It is a costly method in terms of time and efficiency, since it requires a vast amount of mutants to be generated. For this reason, mutant generation should be performed automatically with the help of automated tools. There are a number of mutation testing tools available and each one is supported by a single programming language. In this study, we analyze mutation testing tools for C#. We focus on different characteristics of the tools and aim to help the testers when deciding which tool they can use for their implementations by providing a comparative analysis. © 2019 IEEE.","C# programming language; mutation operators; mutation testing; software testing","Ada (programming language); Application programs; Quality control; Testing; Artificial defects; Automated tools; C# programming; Comparative analysis; Fault-based; Mutation operators; Mutation testing; Software testing techniques; Software testing","Institute of Electrical and Electronics Engineers Inc."
"Saifan A.A., Ata M.B.","Mutation Testing for Evaluating PHP Web Applications",2019,"International Journal of Software Innovation","7","4",,"25","50",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072184598&doi=10.4018%2fIJSI.2019100102&partnerID=40&md5=2cd8adfcfd67d8501cc1237ab427f868","Web applications provide services to hundreds of billions of people over the world, so they should be tested, to insure their validity. In this article, we are investigating the ability of testing web application based on traditional mutation testing. To perform this test, we have defined 54 mutation operators, classified into six categories: SQL data retrieving, data manipulation; domain name and IP address look up; internet protocol and service information; HTTP; connection to server and to database. The test was applied to websites that are built using PHP programming for two reasons. The majority of websites nowadays are built using ASP.net or PHP and most of the testing efforts that have been applied on web applications were using the Java programming language. We have implemented a prototype tool called μWebPHP for automatically generating mutants for PHP web applications based on the identified mutation operators. We report preliminary results that show that mutation testing is feasible for web applications. © 2019, IGI Global.","Equivalent Mutant; Killed Mutants; Mutation Operator; Mutation Score; Web Application Testing",,"Taru Publications"
"Tariq M.U., Bashir M.B., Babar M., Sohail A.","Code readability management of high-level programming languages: A comparative study",2020,"International Journal of Advanced Computer Science and Applications","11","3",,"595","602",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083190775&partnerID=40&md5=a4ee50a6a043cf0fccd3e50d45c95893","Quality can never be an accident and therefore, software engineers are paying immense attention to produce quality software product. Source code readability is one of those important factors that play a vital role in producing quality software. The code readability is an internal quality attribute that directly affects the future maintenance of the software and reusability of same code in similar other projects. Literature shows that readability does not just rely on programmer's ability to write tidy code but it also depends on programming language's syntax. Syntax is the most visible part of any programming language that directly influence the readability of its code. If readability is a major factor for a given project, the programmers should know about the language that they shall choose to achieve the required level of quality. For this we compare the readability of three most popular high-level programming languages; Java, C#, and C++. We propose a comprehensive framework for readability comparison among these languages. The comparison has been performed on the basis of certain readability parameters that are referenced in the literature. We have also implemented an analysis tool and performed extensive experiments that produced interesting results. Furthermore, to judge the effectiveness of these results, we have performed statistical analysis using SPSS (Statistical Package for Social Sciences) tool. We have chosen the Spearman's correlation ad Mann Whitney's T-test for the same. The results show that among all three languages, Java has the most readable code. Programmers should use Java in the projects that have code readability as a significant quality requirement. © 2020, Science and Information Organization.","C#; C++; Code readability; Code readability index; High-level programming languages; Java; Source code","C++ (programming language); Codes (symbols); Color; Computer software reusability; Java programming language; Syntactics; C#; C++; Code readability; Code readability index; High-level programming language; Higher-level programming languages; Java; Quality software; Source codes; Reusability","Science and Information Organization"
"Kausar S., Huahu X., Ullah A., Wenhao Z., Shabir M.Y.","Fog-Assisted Secure Data Exchange for Examination and Testing in E-learning System",2020,"Mobile Networks and Applications",,,,"","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078235193&doi=10.1007%2fs11036-019-01429-x&partnerID=40&md5=9be4be656fcb381b3aa02ffcb56c0735","E-learning systems are getting growing interest due to their wide applicability in distance education. A huge amount of data is shared among students, teachers, examiners that should be exchanged in a confidential manner. In literature, a number of related clustering-based schemes are explored that consider security but still there is a need for dependable secure schemes. This paper explores a Secure E-learning System (SES) for sharing examinations related materials by ensuring protection against various security attacks. Exam materials include tests, quizzes, question papers, answer sheets, and aptitude tests. In the first phase, we present a secure authentication mechanism for students and teachers with a trusted server or a fog server. Next, we present a Session Key Establishment Protocol (SKEP) to setup keys for a specified time period such as a class, seminar or exam. We have also maintained the level of trust and authentication level to regularly check the legitimacy of the students. A security analysis is performed to highlight the pros and cons of security schemes to ensure reliable security for e-learning systems. We have setup a testbed using web-services in ASP.net and C# on windows Azure cloud for an e-learning scenario. Results demonstrate the effectiveness of the proposed SES in terms of reducing number of untrusted students, exams exposed, student interaction time, authentication level, reputation and trust levels for students. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","E-learning; Fog computing; Security; Session keys; Storage repositories","Authentication; Digital storage; E-learning; Electronic data interchange; Fog; Fog computing; Students; Web services; Windows operating system; E-learning scenario; Secure authentications; Security; Security analysis; Security attacks; Security scheme; Session key; Student interactions; Learning systems","Springer"
"Dovbysh A., Alieksieiev V.","Development and integration of speech recognition tools into software applications and an approach to improve of speech recognition quality",2020,"Proceedings - 15th International Conference on Advanced Trends in Radioelectronics, Telecommunications and Computer Engineering, TCSET 2020",,,"9088728","614","617",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086309718&doi=10.1109%2fTCSET49122.2020.235505&partnerID=40&md5=5b8b47d51e014bfdc666b3fe2a5aee97","The major problem of embedding third-party software solutions into your own software is that any improvements or upgrades for that third-party software is out of your control. This same problem could be claimed as a major issue for any third-party speech recognition engine that could be used to bring an ability to use voice control features of custom software applications. Among the number of different solutions, either free or commercial, providing a speech recognition there is no any yet giving a 100% quality. The aim of current research is to build and test an approach, that should improve a quality of speech recognition without necessity of making changes in a third-party engine. For the purpose of current research paper, we'd chosen Microsoft Speech Recognition Engine as a core engine to provide feature of speech recognition that could be integrated into a custom software. Next, using Tensorflow, a neural network was trained to provide technique of speakers' diarization from audio stream. Finally, the Levenshtein's algorithm was used to improve speech recognition quality via application of word correction filter developed in C#. For the purpose of test of integration of speech recognition feature into custom software the application was developed also in C#. As a result, the quality of speech recognition for the test dataset was raised up by 16-17% average. © 2020 IEEE.","audio stream filtering; diarization; Levenshtein's algorithm; MS Speech Recognition Engine; neural network; speech recognition; TensorFlow; word correcion","Application programs; Engines; Software quality; Software testing; Speech; Statistical tests; Core engines; Recognition features; Research papers; Software applications; Speech recognition engine; Third parties; Third party software; Voice control; Speech recognition","Institute of Electrical and Electronics Engineers Inc."
"Angelovski D., Stankov E., Jovanov M.","DEMAx Tool Based on an Improved Model for Semiautomatic C/C++ Source Code Assessment",2021,"ACM International Conference Proceeding Series",,,,"68","73",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117948688&doi=10.1145%2f3470716.3470728&partnerID=40&md5=40e7fa8354041326f9da79413a54e50f","As the demand for software engineers rises, so does the demand for their education. With the increasing number of students, educators struggle to keep up. We aim to ease their burden by providing a new tool for semiautomatic source code assessment, named DEMAx. It analyzes C/C++ source codes and their test case results and with the help of machine learning, provides information on the likelihood that a submission should be manually assessed. In this paper we present a tool with the focus on the new improvements of our previous work that include direct static analysis of non-compiling code and ranking metrics of the source codes. At the end, we present the results of the improved model on the testing data, which are solid ground for the use of our tool. © 2021 ACM.","Clustering of source codes; Introductory programming courses; Semiautomatic source code assessment; Static C/C++ source code analysis","C++ (programming language); Education computing; Static analysis; C# source code; Clustering of source code; Clusterings; Introductory programming course; Semiautomatic source code assessment; Source code analysis; Source codes; Static C/C++ source code analyse; Test case; Automation","Association for Computing Machinery"
"Zmaranda D., Pop-Fele L.-L., Gyorodi R., Gyorodi C.","Actor Model versus TPL for applications development",2021,"2021 16th International Conference on Engineering of Modern Electric Systems, EMES 2021 - Proceedings",,,"9484154","","",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113379050&doi=10.1109%2fEMES52337.2021.9484154&partnerID=40&md5=526d98632df4fed14094f378ae03409c","As more and more people are using technology as means to communicate, learn, run businesses or just having fun, there is hidden pressure over the infrastructure that offers all these services to the end users. In order to provide a better service reliability there is a need of using technologies that take advantage of the underlying hardware such as asynchronous libraries which by design are using the hardware resources at their full capabilities. The question here is what asynchronous approach should be used? We can take use of Microsoft's asynchronous library implementation based on tasks manipulation: TPL (Task Parallel Library), create our own asynchronous library based on the ThreadPool or we can use a different approach by taking use of the Actor Model implemented by Akka.NET framework. This paper is making a performance comparison between two different asynchronous patterns implemented in.NET: task-based and actor-based, by comparing the timing results of the two approaches while realizing actions that can be found in any application: reading from database, data manipulation, saving in the database. In order to perform the analysis and to ensure the consistency of tests, two concurrent applications were developed, having the same data source one with the TPL library and the other with Akka.NET. The applications were run on four different architectures with different core and memory configurations; furthermore, a comprehensive analysis and discussion based on the obtained performance results was done and several conclusions were revealed. Advantages and drawbacks for each asynchronous technology were outlined, to support a decision that could be used in current applications. © 2021 IEEE.","Actor Model; Akka.NET; asynchronous programming; concurrency; performance evaluation; TPL - Task Parallel Library","Engineering; Industrial engineering; Applications development; Asynchronous technology; Comprehensive analysis; Data manipulations; Hardware resources; Memory configuration; Performance comparison; Service reliability; Infrastructure as a service (IaaS)","Institute of Electrical and Electronics Engineers Inc."
"Zagorodnyuk S., Sus B., Revenchuk I., Bauzha O.","Software Interaction Problem with the LDAP Directory Service: Description, Resolving, Analysis",2021,"2020 IEEE International Conference on Problems of Infocommunications Science and Technology, PIC S and T 2020 - Proceedings",,,"9467983","117","121",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114410269&doi=10.1109%2fPICST51311.2020.9467983&partnerID=40&md5=f0fe21ba5caae476bccb2a3854bb18ed","During the development of automated software packages for user authorization and granting access to network resources, there is a problem of software identification of the user of the directory service based on the LDAP database to a certain group of users. The standard Microsoft.NET Framework library, designed to perform basic operations on directory service objects, includes a special System.DirectoryServices library. The testing of this library has revealed that this mechanism only works properly with the group up to 1,500 members. This limitation of the System.DirectoryServices library remains uncorrected in all versions of.NET Framework. Wrong detection the group membership can lead to the dangerous situation when a authenticated user gets more large rights and privileges than it should or, on the contrary, does not get the necessary rights. The purpose of the article is to demonstrate how using of the library Novell.Directory.Ldap allows the administrator to produce the new working code for correct membership detection a certain user account in the user group. © 2020 IEEE.","class library; directory service; group membership; LDAP object attributes; user access rights","Chemical detection; Basic operation; Dangerous situations; Directory service; Group memberships; Interaction problems; Microsoft .NET; Network resource; Software identifications; Authentication","Institute of Electrical and Electronics Engineers Inc."
